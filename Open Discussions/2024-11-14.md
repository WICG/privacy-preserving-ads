# Ad Selection API Discussion
Thursday, November 14th  at 11 am EDT /  8 am PDT
Scribe: Pooja Tetali 

Open discussion to review technical details and open GitHub issues for the Ad Selection API proposal.

If you want to participate, please make sure you join the WICG: https://www.w3.org/community/wicg/
## Agenda
- Sign up for the developer preview of the ad-selection api here 
- Open agenda

## Notes:
DD: Google is pursuing IETF (internet engineering task force) for protocols, W3C for APIs. Does ADSAPI have the aspiration to go?   

BM: We talked about what this looks like in W3C. Forward looking it makes sense to have a common language with some differences (i.e user agent) MK what are your thoughts? 

MK: Paul and other Chrome standards related folks are here. It is easy to draw the line between, on the one hand stuff that needs to match — the API in the browser and the wire protocol of how the browser talks to servers — and on the other hand what actually happens inside servers themselves.  The browser and wire protocol are areas where we really need things to match.  The differences between the two browsers are mostly regarding servers. The difference between selectAd (singular in Chrome) and selectAds (plural in Edge) is something we should probably spend more time on because maybe it leads to a difference in browser protocol. Figuring out if that is something we can standardize (returning multiple ads or single ads) is a good area to investigate. The question of what is actually allowed for server side components are outside the view of a lot of standardization organizations. We want to make it easier for devs using the API to do as little work as possible but that mostly on us rather than standards organizations.   


IF: To dig into this- the boundary between client/server side is somewhat clear:  What type of binary/runtime allowed? Stuff in between: topologies allowed. Ultimately the browser releases into the environment. Would that browser decision make sense for WICG and would the other stuff be better in IETF?   


MK: Community groups make stuff that moves to working groups and eventually ends up on a standard track. When we talk about something that would become a standard. The standard describes behavior of the browser. So from the question: what server side components is the browser willing to send to. From the standards POV: cryptographic keys (server side components) which is a policy sort of decision and there is no way a standards group can say that. I don’t see how that would be in bounds.   


IF: There are some RFCs in an environment that will continue to get developed. Would you see it as a browser making a decision to release the key to the test environment based on “these are TEE’s..” as an input to the decision making process?   


MK: No. There are 2 different questions: 1. What is a TEE? (A piece of infrastructure that lets you do some sort of remote attestation about what binary gets running, plus some sort of hardening), and 2. Rules: some sort of decision about what that binary should be able to do, i.e. what processing is allowed. So even if there are standards (IETF etc) around one of those 2 things. That's a very different environment. What are the rules is a different issue from whether you have a computer that can enforce the rules.  


IF: a) I am releasing keys to the environment b) what are the rules. I am trying to dig at your thinking- what body a and b belong in. Hopefully we agree that we can pursue commonality on a or b. I can’t say if this will be in a published W3C doc?   


MK: When you are talking about browsers making a decision on what keys to use…  


IF: I wonder if I am saying something silly. I am not saying that keys need to be the same. For b) what are the rules- given that we trust this thing will execute reliably what are the ways this can happen so ad techs can implement a common solution. It won’t be a W3C or maybe an IETF but its a common agreement amongst devs. For a) the protocol we use to determine if hardware is proper, there are RFCs around it. The way we do that does stay consistent.   


MK: I'm trying to find something else that browsers already do that gets at these same sorts of questions. What comes to me naturally, is the root certificate authorities. When the browser goes to https://www.microsoft.com, how does the browser know that it is a microsoft site?  It looks at a cryptographic certificate. The browser sees the certificate was issued by a chain of issuers and ultimately gets back to some Root Certificate Authority: “This is a party that is trustworthy to handle issuing cryptographic certificates to the person they are supposed to be issued to.” This is a lot of technical work that roots to the browser putting a lot of faith and trust into something that verifies that it is indeed true. There is a list of root certificate authorities that the major browsers have agreed on not because it is defined by a standard body but because they have agreed on this. When a root certificate authority is no longer considered ok, it is not removed in sync from all browsers- it takes time. Every browser makes its own conclusion/decision. This is not the best solution, but it resembles something like the boundary of technical enforcement that they do today.  


DD: IF, it sounds like there is still some discussion, but I picked up that the view of the world is that browsers can standardize what the api is, behavior of API. But IF is looking for:  from this POV I need to use this but I have expectations from this browser API of further APIs that are supported rather than just a partial system.   


MK: That is accurate- to take something else analogous: Work that is happening in PATCG/PATWG for the aggregate attribution API. This has a lot of support across many browsers- joint effort from Chrome, Firefox and Safari, who are working on commonality. But again, eventually that all turns into- now we have data that we are going to encrypt using some keys, which keys are you willing to use to encrypt it. Safari and Firefox have both indicated they're using secure multi-party computation.  No need to go into technical details, but the point is that the data gets broken up into multiple pieces, each individual piece doesn't reveal anything, each of those pieces sent to a different helper party, and as long as there is no collusion, the user's data is kept private.  This is great from a cryptography POV. But ultimately the browser has to agree that the people who run the different helper parties aren't colluding, which is a judgment call that browsers have to make. That's the kind of thing that is outside from what standard calls can determine. I don’t see a way to get away from that for privacy related APIs that involve server side components.   


DD: Following on from certificate authorities. User agents are gonna say- despite standardization I don’t trust this CA. There will still be judgment calls.   


IF: It sounds like when we decouple a number of different things-  Make things easier for ad tech by putting it into a body and can pressure be applied through that body; the framework vs attestation process. The people in this room want the first question (do we want this to be consistent) and we at MSFT want this. MK it sounds like what you're saying on the question of is there an appropriate body: is there a guide? 

MK: There is not a body that fulfills that roll. For CAs, the browsers and certificate authorities banded together and created a new body (CABF) and it was something that they had to invent to establish commonality. It was a first attempt, I think in retrospect it has good parts and bad parts. People seem to agree that if it was implemented today it would look different. That shared judgment call would need to be in the hands of browsers or something else. Standards organizations like the W3C or IETF would not fulfill what you are saying. 

DD: The direction private attribution is saying doesn’t have this issue ahead of it. I haven’t heard anything: you have to run this on a TEE. Is that because everything or the properties of the system are all bundled up in protocols because everything is specified so there are no constraints. 

MK: No, exactly the opposite. All of the questions we're asking here, about what is an acceptable server side component, is a question that the private attribution work is going to need to deal with, in the same way we are dealing with it. TEE vs multi-computation- doesn’t change the fact that the browser needs to make the decision: I am handing off the data because I trust how it is using the data. I don’t believe that group has yet worked on the question Isaac is asking. 

DD: Browsers control the means of production 

IF: Going back to framing  differences: Are these browsers trying to make this consistent?Is there anything constraining it? What type of environment are we willing to release into? (domain specific) What you are saying: browsers can choose to accept environments defined by some other standards body and that's the choice they can make and we should strive to have some kind of consistency between them. For domain specific rules: there could theoretically be a body that makes that decision but even though that exists for browsers that thing doesn’t exist for servers, so it wouldn’t be enforced. 

MK: Standard bodies don’t enforce,  they specify. I don’t know anything analogous for server side components. 

DD: The web protocol, which says things like HTTP GET requests being idempotent

MK: The web specification doesn’t say about the internal processing via server. 

MK: Idempotence is a very interesting example. That's about what happens when a GET request hits a server multiple times, should have the same effect as if it reached the server a single time.  David is right that this is a statement that the HTTP protocol makes about how it believes how servers ought to be be implemented. But why does that spec talk about server behavior? The point is- that specification is focused on what browsers can do, so the spec is saying that if a browser runs into some kind of problem with a GET request, then it's okay for the browser to reissue the request. The spec is warning servers that if they can't handle a GET being received twice, they might be in trouble because browsers might re-send and it would make them unhappy.  This is not a constraint saying that  browsers shouldn't talk to servers if there is a server that doesn’t handle this scenario correctly. It is an example of a spec that details what is happening in the protocol/server but doesn’t enforce what actually happens. Privacy needs about how the server behaves is a different world, different from how traditional web standards approach server side constraints. 

DD: Even if it were apple and chrome those percentages is a non starter. 

LB: We will have different clusters for each browser. This will be a challenge   

MK: The work in PATCG about aggregation and measurement would be a good place to have this conversation because that work is across different browsers and vendors. They are further along, but we will get there in this effort too. 

DD: I hope that this effort remains a community effort because if it follows the privacy CG trend it would be a “pay to play”. The privacy CG is no longer community group, in the past anyone could come and participate. Now that its a working group- unless your company pays you can’t participate in that working group. I hope this remains an open group. 

MK: Sounds like your concern is regarding the rules of the W3C. Standards come from working groups not community groups. 

MK: There might be other ways standardization can work for what we need. 

IF: Does the framing of this as- do humans in this room want to make these ADSAPI/ PA compatible from an infrastructural and logic/flow of the data  POV? The concern is how do we ensure that in the future it remains publicly, discussed,  encouraged, specced. How do we maintain this going forward? 

DD: I guess- a minor difference in capabilities in usage for what Edge makes available for 5-6% market share vs Chrome? There is a juice-squeeze issue. I don’t know if I answered your question. 

IF: Market forces vs government forces. For Edge, we made a deliberate decision- if we do something different adoption would be rough. We can rely on browsers making self interested decisions to keep these things working together. Is there a separate motivator? Is there a way to constrain that in a more formal way instead of “hey these things happen to align” 

DD: We are just talking about targeting- there is the measurement piece, I suppose if there is some convergence. But are any browsers going to present which is a coefficient of difference? 

IF: The question applies equally well to the targeting work of PATCG and attribution work. But for the sake of conversation let's pretend Edge says we will support ARA. The same questions and decoupling applies. Relying on world forces (market, govt) and another force (standards body etc) to guide that- that specific one we do not have an answer today. 
